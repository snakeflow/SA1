{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SA1 scPDSI Processing - Testing Notebook\n",
    "\n",
    "This notebook allows you to test the scPDSI processor with your own sample data.\n",
    "Fill in the parameters below and run the cells to process your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Parameters\n",
    "\n",
    "**Please fill in your specific paths and parameters below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Basic parameters ---\n",
    "basic_path = r''  # Your base directory path\n",
    "project_name = ''  # Your project folder name\n",
    "variable_name = ''  # NetCDF variable name (e.g., 'scpdsi')\n",
    "x_dimension = ''  # Longitude dimension name\n",
    "y_dimension = ''  # Latitude dimension name\n",
    "time_dimension = ''  # Time dimension name\n",
    "zone_field = \"\"  # SA1 code field name\n",
    "\n",
    "# --- Paths ---\n",
    "nc_path = f\"\"  # NetCDF files directory\n",
    "work_path = f\"\"  # Temporary work directory\n",
    "save_excel_path = f\"\"  # Excel output directory\n",
    "shp_path = f\"\"  # Shapefile directory\n",
    "shp_filename = \"\"  # Shapefile name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom processor\n",
    "from scpdsi_processor import (\n",
    "    Config, ScPDSIProcessor, Logger, ProcessingError,\n",
    "    SA1DataManager, NetCDFManager, GridManager,\n",
    "    AnnualAggregator, ReprojectionManager, ConsistencyChecker, ExcelExporter\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Update Configuration with Your Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update configuration with your parameters\n",
    "if basic_path and project_name:\n",
    "    # Update paths\n",
    "    Config.NETCDF_DIR = nc_path\n",
    "    Config.OUTPUT_DIR = save_excel_path\n",
    "    Config.TEMP_DIR = work_path\n",
    "    Config.LOG_DIR = os.path.join(basic_path, project_name, \"logs\")\n",
    "    Config.SA1_SHAPEFILE = os.path.join(shp_path, shp_filename)\n",
    "    \n",
    "    # Update variable and dimension names\n",
    "    if variable_name:\n",
    "        Config.VARIABLE_NAME = variable_name\n",
    "    if x_dimension:\n",
    "        Config.LON_DIM = x_dimension\n",
    "    if y_dimension:\n",
    "        Config.LAT_DIM = y_dimension\n",
    "    if time_dimension:\n",
    "        Config.TIME_DIM = time_dimension\n",
    "    if zone_field:\n",
    "        Config.SA1_KEY = zone_field\n",
    "    \n",
    "    print(\"‚úÖ Configuration updated with your parameters\")\n",
    "    \n",
    "    # Display current configuration\n",
    "    print(\"\\nüìã Current Configuration:\")\n",
    "    print(f\"  NetCDF Directory: {Config.NETCDF_DIR}\")\n",
    "    print(f\"  Output Directory: {Config.OUTPUT_DIR}\")\n",
    "    print(f\"  SA1 Shapefile: {Config.SA1_SHAPEFILE}\")\n",
    "    print(f\"  Variable Name: {Config.VARIABLE_NAME}\")\n",
    "    print(f\"  Dimensions: {Config.LON_DIM}, {Config.LAT_DIM}, {Config.TIME_DIM}\")\n",
    "    print(f\"  SA1 Key Field: {Config.SA1_KEY}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please fill in the basic_path and project_name parameters above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check Input Files and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if directories and files exist\n",
    "def check_inputs():\n",
    "    print(\"üîç Checking input files and directories...\\n\")\n",
    "    \n",
    "    # Check NetCDF directory\n",
    "    nc_dir = Path(Config.NETCDF_DIR)\n",
    "    if nc_dir.exists():\n",
    "        nc_files = list(nc_dir.glob(\"*.nc\"))\n",
    "        print(f\"‚úÖ NetCDF directory found: {nc_dir}\")\n",
    "        print(f\"   üìÅ Contains {len(nc_files)} NetCDF files\")\n",
    "        if nc_files:\n",
    "            print(f\"   üìÑ First few files: {[f.name for f in nc_files[:3]]}\")\n",
    "    else:\n",
    "        print(f\"‚ùå NetCDF directory not found: {nc_dir}\")\n",
    "    \n",
    "    # Check SA1 shapefile\n",
    "    shp_file = Path(Config.SA1_SHAPEFILE)\n",
    "    if shp_file.exists():\n",
    "        print(f\"‚úÖ SA1 shapefile found: {shp_file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå SA1 shapefile not found: {shp_file}\")\n",
    "    \n",
    "    # Check/create output directories\n",
    "    for dir_name, dir_path in [(\"Output\", Config.OUTPUT_DIR), \n",
    "                               (\"Temp\", Config.TEMP_DIR), \n",
    "                               (\"Logs\", Config.LOG_DIR)]:\n",
    "        dir_obj = Path(dir_path)\n",
    "        if not dir_obj.exists():\n",
    "            dir_obj.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"üìÅ Created {dir_name} directory: {dir_path}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {dir_name} directory exists: {dir_path}\")\n",
    "\ncheck_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspection of the first NetCDF file\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "def inspect_data():\n",
    "    print(\"üîé Inspecting sample data...\\n\")\n",
    "    \n",
    "    # Inspect NetCDF file\n",
    "    nc_files = list(Path(Config.NETCDF_DIR).glob(\"*.nc\"))\n",
    "    if nc_files:\n",
    "        print(f\"üìä Inspecting first NetCDF file: {nc_files[0].name}\")\n",
    "        try:\n",
    "            with xr.open_dataset(nc_files[0]) as ds:\n",
    "                print(f\"   üìê Dimensions: {dict(ds.dims)}\")\n",
    "                print(f\"   üìä Variables: {list(ds.data_vars.keys())}\")\n",
    "                print(f\"   üïê Time range: {ds[Config.TIME_DIM].values[0]} to {ds[Config.TIME_DIM].values[-1]}\")\n",
    "                \n",
    "                if Config.VARIABLE_NAME in ds.data_vars:\n",
    "                    var_data = ds[Config.VARIABLE_NAME]\n",
    "                    print(f\"   ‚úÖ Target variable '{Config.VARIABLE_NAME}' found\")\n",
    "                    print(f\"   üìä Shape: {var_data.shape}\")\n",
    "                    print(f\"   üéØ Data range: {float(var_data.min()):.3f} to {float(var_data.max()):.3f}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Target variable '{Config.VARIABLE_NAME}' not found\")\n",
    "                    print(f\"   üìä Available variables: {list(ds.data_vars.keys())}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error reading NetCDF: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    \n",
    "    # Inspect shapefile\n",
    "    shp_file = Path(Config.SA1_SHAPEFILE)\n",
    "    if shp_file.exists():\n",
    "        print(f\"üó∫Ô∏è  Inspecting shapefile: {shp_file.name}\")\n",
    "        try:\n",
    "            gdf = gpd.read_file(shp_file)\n",
    "            print(f\"   üìä Number of features: {len(gdf)}\")\n",
    "            print(f\"   üìä Columns: {list(gdf.columns)}\")\n",
    "            print(f\"   üåç CRS: {gdf.crs}\")\n",
    "            \n",
    "            if Config.SA1_KEY in gdf.columns:\n",
    "                print(f\"   ‚úÖ Target field '{Config.SA1_KEY}' found\")\n",
    "                print(f\"   üìä Sample values: {gdf[Config.SA1_KEY].head(3).tolist()}\")\n",
    "                print(f\"   üî¢ Unique values: {gdf[Config.SA1_KEY].nunique()}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå Target field '{Config.SA1_KEY}' not found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error reading shapefile: {str(e)}\")\n",
    "\ninspect_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Individual Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SA1 data loading\n",
    "print(\"üß™ Testing SA1 data loading...\")\n",
    "\n",
    "try:\n",
    "    sa1_manager = SA1DataManager()\n",
    "    sa1_gdf = sa1_manager.load_sa1_data()\n",
    "    print(f\"‚úÖ SA1 data loaded successfully: {len(sa1_gdf)} regions\")\n",
    "    \n",
    "    # Test reprojection\n",
    "    sa1_reprojected = sa1_manager.reproject_to_target_crs()\n",
    "    print(f\"‚úÖ SA1 data reprojected to {Config.TARGET_CRS}\")\n",
    "    \n",
    "    # Test centroids\n",
    "    centroids = sa1_manager.create_centroids()\n",
    "    print(f\"‚úÖ SA1 centroids created: {len(centroids)} points\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå SA1 testing failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test NetCDF data loading\n",
    "print(\"üß™ Testing NetCDF data loading...\")\n",
    "\n",
    "try:\n",
    "    netcdf_manager = NetCDFManager()\n",
    "    files = netcdf_manager.discover_netcdf_files()\n",
    "    print(f\"‚úÖ NetCDF files discovered: {len(files)} files\")\n",
    "    \n",
    "    # Test reading first few monthly slices\n",
    "    monthly_data = netcdf_manager.read_monthly_data()\n",
    "    print(f\"‚úÖ Monthly data loaded: {len(monthly_data)} time slices\")\n",
    "    \n",
    "    # Test year grouping\n",
    "    calendar_years, financial_years = netcdf_manager.group_by_years()\n",
    "    print(f\"‚úÖ Year grouping completed:\")\n",
    "    print(f\"   üìÖ Calendar years: {len(calendar_years)} ({list(calendar_years.keys())})\")\n",
    "    print(f\"   üí∞ Financial years: {len(financial_years)} ({list(financial_years.keys())})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå NetCDF testing failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Complete Processing (Small Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test with limited data (first year only)\n",
    "print(\"üöÄ Running complete processing test...\\n\")\n",
    "\n",
    "# Setup logging for this test\n",
    "logger_setup = Logger(logging.INFO)\n",
    "\n",
    "try:\n",
    "    # Create processor instance\n",
    "    processor = ScPDSIProcessor()\n",
    "    \n",
    "    # Run complete workflow\n",
    "    processor.run()\n",
    "    \n",
    "    print(\"\\nüéâ Processing completed successfully!\")\n",
    "    \n",
    "    # Check outputs\n",
    "    output_dir = Path(Config.OUTPUT_DIR)\n",
    "    excel_files = list(output_dir.glob(\"*.xlsx\"))\n",
    "    print(f\"\\nüìÑ Generated Excel files: {len(excel_files)}\")\n",
    "    for excel_file in excel_files:\n",
    "        file_size_kb = excel_file.stat().st_size / 1024\n",
    "        print(f\"   üìä {excel_file.name} ({file_size_kb:.1f} KB)\")\n",
    "    \n",
    "except ProcessingError as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Processing error (expected in some cases): {str(e)}\")\n",
    "    print(\"This might be due to consistency check failures or data alignment issues.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Unexpected error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect generated Excel files\n",
    "import pandas as pd\n",
    "\n",
    "def inspect_results():\n",
    "    print(\"üìä Inspecting generated results...\\n\")\n",
    "    \n",
    "    output_dir = Path(Config.OUTPUT_DIR)\n",
    "    excel_files = list(output_dir.glob(\"*.xlsx\"))\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(\"‚ùå No Excel files found in output directory\")\n",
    "        return\n",
    "    \n",
    "    for excel_file in excel_files[:2]:  # Inspect first 2 files\n",
    "        print(f\"üìÑ Inspecting: {excel_file.name}\")\n",
    "        try:\n",
    "            df = pd.read_excel(excel_file)\n",
    "            print(f\"   üìä Shape: {df.shape}\")\n",
    "            print(f\"   üìä Columns: {list(df.columns)}\")\n",
    "            print(f\"   üìä Sample data:\")\n",
    "            print(df.head(3).to_string(index=False))\n",
    "            \n",
    "            # Check data quality\n",
    "            if 'scpdsi_mean' in df.columns:\n",
    "                valid_values = df['scpdsi_mean'].notna().sum()\n",
    "                print(f\"   ‚úÖ Valid scPDSI values: {valid_values}/{len(df)} ({100*valid_values/len(df):.1f}%)\")\n",
    "                \n",
    "                if valid_values > 0:\n",
    "                    mean_val = df['scpdsi_mean'].mean()\n",
    "                    min_val = df['scpdsi_mean'].min()\n",
    "                    max_val = df['scpdsi_mean'].max()\n",
    "                    print(f\"   üìä scPDSI range: {min_val:.3f} to {max_val:.3f} (mean: {mean_val:.3f})\")\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error reading Excel file: {str(e)}\")\n",
    "\ninspect_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Check Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recent log entries\n",
    "def show_recent_logs():\n",
    "    print(\"üìã Recent log entries...\\n\")\n",
    "    \n",
    "    log_dir = Path(Config.LOG_DIR)\n",
    "    log_files = sorted(log_dir.glob(\"*.log\"))\n",
    "    \n",
    "    if not log_files:\n",
    "        print(\"‚ùå No log files found\")\n",
    "        return\n",
    "    \n",
    "    # Get the most recent log file\n",
    "    latest_log = log_files[-1]\n",
    "    print(f\"üìÑ Latest log file: {latest_log.name}\\n\")\n",
    "    \n",
    "    try:\n",
    "        with open(latest_log, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # Show last 20 lines\n",
    "        print(\"üìÑ Last 20 log entries:\")\n",
    "        print(\"-\" * 80)\n",
    "        for line in lines[-20:]:\n",
    "            print(line.rstrip())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading log file: {str(e)}\")\n",
    "\nshow_recent_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "def show_summary():\n",
    "    print(\"üìã PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Count outputs\n",
    "    output_dir = Path(Config.OUTPUT_DIR)\n",
    "    excel_files = list(output_dir.glob(\"*.xlsx\"))\n",
    "    calendar_files = [f for f in excel_files if 'calendar' in f.name]\n",
    "    financial_files = [f for f in excel_files if 'financial' in f.name]\n",
    "    \n",
    "    print(f\"üìä Total Excel files generated: {len(excel_files)}\")\n",
    "    print(f\"   üìÖ Calendar years: {len(calendar_files)}\")\n",
    "    print(f\"   üí∞ Financial years: {len(financial_files)}\")\n",
    "    \n",
    "    # Check log files\n",
    "    log_dir = Path(Config.LOG_DIR)\n",
    "    log_files = list(log_dir.glob(\"*.log\"))\n",
    "    validation_reports = list(log_dir.glob(\"validation_report_*.json\"))\n",
    "    \n",
    "    print(f\"üìã Log files: {len(log_files)}\")\n",
    "    if validation_reports:\n",
    "        print(f\"‚ö†Ô∏è  Validation reports: {len(validation_reports)} (consistency check failures)\")\n",
    "    \n",
    "    print(\"\\nüéØ NEXT STEPS:\")\n",
    "    print(\"1. Review the generated Excel files in:\", Config.OUTPUT_DIR)\n",
    "    print(\"2. Check log files for any warnings or errors in:\", Config.LOG_DIR)\n",
    "    if validation_reports:\n",
    "        print(\"3. Review validation reports for consistency check failures\")\n",
    "    print(\"4. Adjust configuration parameters if needed\")\n",
    "    print(\"5. Run with full dataset when satisfied with test results\")\n",
    "\nshow_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}